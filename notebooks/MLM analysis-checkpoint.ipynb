{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72671f9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/shubhobm/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|████████████████████████████████| 8.17k/8.17k [00:00<00:00, 7.66MB/s]\n",
      "Downloading readme: 100%|████████████████████████████████████████| 7.44k/7.44k [00:00<00:00, 99.4kB/s]\n",
      "No config specified, defaulting to: bias-shades/english\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset bias-shades/english to /home/shubhobm/.cache/huggingface/datasets/BigScienceBiasEval___bias-shades/english/0.0.1/0f168db8c9cb6705c8d5362fac6ad8e30261cb4e90896937f33813cbfe4c3e59...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|████████████████████████████████████████████| 361k/361k [00:09<00:00, 37.3kB/s]\n",
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset bias-shades downloaded and prepared to /home/shubhobm/.cache/huggingface/datasets/BigScienceBiasEval___bias-shades/english/0.0.1/0f168db8c9cb6705c8d5362fac6ad8e30261cb4e90896937f33813cbfe4c3e59. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 318.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# !pip install -qq transformers plotly\n",
    "# !pip install -qq datasets evaluate honest\n",
    "# load datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"BigScienceBiasEval/bias-shades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d27d107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 12:25:18.539131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-09 12:25:19.340789: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-09 12:25:19.340828: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-09 12:25:19.425891: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-09 12:25:20.744888: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-09 12:25:20.744990: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-09 12:25:20.744998: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Downloading: 100%|████████████████████████████████████████████████████| 615/615 [00:00<00:00, 631kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████| 5.07M/5.07M [01:32<00:00, 54.7kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████| 9.10M/9.10M [01:42<00:00, 89.0kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████| 1.12G/1.12G [01:12<00:00, 15.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e258eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "def generate_sentences(cloze_phrase, bias_pronoun, anti_bias_pronoun):\n",
    "    biased_phrase = cloze_phrase.replace('[MASK]', bias_pronoun)\n",
    "    antibiased_phrase = cloze_phrase.replace('[MASK]', anti_bias_pronoun)\n",
    "    return (biased_phrase, antibiased_phrase)\n",
    "\n",
    "def calculate_perplexity(inputlist, mname):\n",
    "    resultsdict={}\n",
    "    perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "    ppl = perplexity.compute(input_texts=inputlist, model_id=mname, add_start_token=False)\n",
    "    return(ppl['perplexities'])\n",
    "\n",
    "def calculate_biases(cloze_phrase, bias_pronoun, anti_bias_pronoun, biased_ppl, anti_biased_ppl):\n",
    "    p_bias = math.pow(1 / biased_ppl, len(cloze_phrase.split()))\n",
    "    p_anti_bias = math.pow(1 / anti_biased_ppl, len(cloze_phrase.split()))\n",
    "    if anti_bias_pronoun in ['she','her','herself']:\n",
    "        f_proba = p_anti_bias\n",
    "        m_proba = p_bias\n",
    "        av_bias = 2 * (m_proba / (f_proba+m_proba) - 0.5)\n",
    "    else:\n",
    "        m_proba = p_anti_bias\n",
    "        f_proba = p_bias\n",
    "        av_bias = 2 * (f_proba / (f_proba+m_proba) - 0.5)\n",
    "    m_bias = 2 * (m_proba / (f_proba+m_proba) - 0.5)\n",
    "    f_bias = 2 * (f_proba / (f_proba+m_proba) - 0.5)\n",
    "    av_bias = max(0, av_bias)\n",
    "    return(p_bias, p_anti_bias, m_bias, f_bias, av_bias)\n",
    "\n",
    "def calculate_mlm_bias(cloze_phrase, bias_p, anti_bias_p, mname):\n",
    "  f_bias = 0.0\n",
    "  m_bias = 0.0\n",
    "  if 'roberta' in mname.model.name_or_path:\n",
    "      preds = mname(cloze_phrase.replace('[MASK]', '<mask>'))\n",
    "  else:\n",
    "      preds = mname(cloze_phrase)\n",
    "  pred_toks = [i['token_str'].strip() for i in preds]\n",
    "  if anti_bias_p in pred_toks:\n",
    "    logit_anti_bias = [i['score'] for i in preds if i['token_str'].strip() == anti_bias_p][0]\n",
    "  else:\n",
    "    logit_anti_bias = 0.0\n",
    "  if bias_p in pred_toks:\n",
    "    logit_bias = [i['score'] for i in preds if i['token_str'].strip() == bias_p][0]\n",
    "  else:\n",
    "    logit_bias = 0.0\n",
    "  if anti_bias_p in ['she','her','herself']:\n",
    "    f_proba = 1 / (1 + math.exp(-logit_anti_bias))\n",
    "    m_proba = 1 / (1 + math.exp(-logit_bias))\n",
    "    av_bias = 2 * (m_proba / (f_proba+m_proba) - 0.5)\n",
    "  else:\n",
    "    m_proba =  1 / (1 + math.exp(-logit_anti_bias))\n",
    "    f_proba = 1 / (1 + math.exp(-logit_bias))\n",
    "    av_bias = 2 * (f_proba / (f_proba+m_proba) - 0.5)\n",
    "  m_bias = 2 * (m_proba / (f_proba+m_proba) - 0.5)\n",
    "  f_bias = 2 * (f_proba / (f_proba+m_proba) - 0.5)\n",
    "  av_bias = max(0, av_bias)\n",
    "  return(m_bias, f_bias, av_bias)\n",
    "\n",
    "def calculate_clm_bias(winodset, mname):\n",
    "    winodset[['biased_phrase','anti_biased_phrase']]  = winodset.apply(lambda row: generate_sentences(row['cloze_phrase'],row['bias_pronoun'],row['anti_bias_pronoun']), axis=1, result_type=\"expand\")\n",
    "    biased_list = winodset['biased_phrase'].tolist()\n",
    "    unbiased_list = winodset['anti_biased_phrase'].tolist()\n",
    "    winodset['biased_ppl']  =  calculate_perplexity(biased_list, mname)\n",
    "    winodset['anti_biased_ppl']  =  calculate_perplexity(unbiased_list, mname)\n",
    "    winodset[['p_bias','p_anti_bias', 'm_bias','f_bias', 'av_bias']]  = winodset.apply(lambda row: calculate_biases(row['cloze_phrase'],row['bias_pronoun'],row['anti_bias_pronoun'], row['biased_ppl'], row['anti_biased_ppl']), axis=1, result_type=\"expand\")\n",
    "    return(winodset)\n",
    "\n",
    "def calculate_wino_bias(modelname, modeltype):\n",
    "    winopath = modelname.replace('/','')+'_winobias.csv'\n",
    "    if Path(winopath).is_file():\n",
    "        print(\"loading local data\")\n",
    "        results_df = pd.read_csv(winopath)\n",
    "    else:\n",
    "        winobias1 = load_dataset(\"sasha/wino_bias_cloze1\", split=\"test\")\n",
    "        winobias2 = load_dataset(\"sasha/wino_bias_cloze2\", split= \"test\")\n",
    "        wino1_df = pd.DataFrame(winobias1)\n",
    "        wino2_df = pd.DataFrame(winobias2)\n",
    "        results_df= pd.concat([wino1_df, wino2_df], axis=0)\n",
    "        if modeltype == \"MLM\":\n",
    "            print(\"Loading MLM!\")\n",
    "            unmasker = pipeline('fill-mask', model=modelname, top_k=10)\n",
    "            results_df[['m_bias','f_bias', 'av_bias']] = results_df.apply(lambda x: calculate_mlm_bias(x.cloze_phrase, x.bias_pronoun, x.anti_bias_pronoun, unmasker), axis=1, result_type=\"expand\")\n",
    "            results_df.to_csv(winopath)\n",
    "        elif modeltype == \"CLM\":\n",
    "            print(\"Loading CLM!\")\n",
    "            results_df= calculate_clm_bias(results_df,modelname)\n",
    "            results_df.to_csv(winopath)\n",
    "    return(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58141c85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading metadata: 100%|██████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 1.59MB/s]\n",
      "Using custom data configuration sasha--wino_bias_cloze1-f8cc52d257c95e72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 28.33 KiB, generated: 75.95 KiB, post-processed: Unknown size, total: 104.28 KiB) to /home/shubhobm/.cache/huggingface/datasets/sasha___parquet/sasha--wino_bias_cloze1-f8cc52d257c95e72/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|                                                   | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                   | 0.00/15.3k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████████████████████████| 15.3k/15.3k [00:00<00:00, 57.2kB/s]\u001b[A\n",
      "Downloading data files:  50%|█████████████████████▌                     | 1/2 [00:01<00:01,  1.53s/it]\n",
      "Downloading data: 100%|██████████████████████████████████████████| 13.8k/13.8k [00:00<00:00, 6.25MB/s]\u001b[A\n",
      "Downloading data files: 100%|███████████████████████████████████████████| 2/2 [00:02<00:00,  1.47s/it]\n",
      "Extracting data files: 100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 1630.76it/s]\n",
      "                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/shubhobm/.cache/huggingface/datasets/sasha___parquet/sasha--wino_bias_cloze1-f8cc52d257c95e72/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading metadata: 100%|██████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 1.37MB/s]\n",
      "Using custom data configuration sasha--wino_bias_cloze2-65beec9c8b1634ff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 28.35 KiB, generated: 71.42 KiB, post-processed: Unknown size, total: 99.76 KiB) to /home/shubhobm/.cache/huggingface/datasets/sasha___parquet/sasha--wino_bias_cloze2-65beec9c8b1634ff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|                                                   | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data: 100%|██████████████████████████████████████████| 14.8k/14.8k [00:00<00:00, 7.59MB/s]\u001b[A\n",
      "Downloading data files:  50%|█████████████████████▌                     | 1/2 [00:01<00:01,  1.16s/it]\n",
      "Downloading data: 100%|██████████████████████████████████████████| 14.3k/14.3k [00:00<00:00, 7.71MB/s]\u001b[A\n",
      "Downloading data files: 100%|███████████████████████████████████████████| 2/2 [00:02<00:00,  1.28s/it]\n",
      "Extracting data files: 100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 1436.65it/s]\n",
      "                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/shubhobm/.cache/huggingface/datasets/sasha___parquet/sasha--wino_bias_cloze2-65beec9c8b1634ff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Loading MLM!\n"
     ]
    }
   ],
   "source": [
    "roberta_eval=calculate_wino_bias(\"xlm-roberta-base\",\"MLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06473565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration sasha--wino_bias_cloze1-f8cc52d257c95e72\n",
      "Found cached dataset parquet (/home/shubhobm/.cache/huggingface/datasets/sasha___parquet/sasha--wino_bias_cloze1-f8cc52d257c95e72/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Using custom data configuration sasha--wino_bias_cloze2-65beec9c8b1634ff\n",
      "Found cached dataset parquet (/home/shubhobm/.cache/huggingface/datasets/sasha___parquet/sasha--wino_bias_cloze2-65beec9c8b1634ff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MLM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████| 570/570 [00:00<00:00, 828kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████| 440M/440M [03:31<00:00, 2.08MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|█████████████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 40.2kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████| 232k/232k [00:01<00:00, 128kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████| 466k/466k [00:05<00:00, 92.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "bert_eval=calculate_wino_bias(\"bert-base-uncased\",\"MLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f02e53cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05651338875079416\n",
      "0.042748975129388425\n"
     ]
    }
   ],
   "source": [
    "nb = bert_eval.shape[0]\n",
    "print(bert_eval['av_bias'].sum()/nb)\n",
    "\n",
    "nr = roberta_eval.shape[0]\n",
    "print(roberta_eval['av_bias'].sum()/nr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
